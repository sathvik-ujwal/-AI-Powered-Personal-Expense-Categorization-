{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5J8WScevrKD2p5SitcRGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathvik-ujwal/-AI-Powered-Personal-Expense-Categorization-/blob/main/expense_categorization1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxmQIvLt6uLz",
        "outputId": "422e37f9-dfa2-40c2-dafe-c8df545696cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyLGCPKK7BXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ef4ed6-243f-4052-9685-8378a4ea99f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "save_dir = '/content/drive/MyDrive/expense_categorization/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('complex_transactions_faker1.csv')\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x93wn_fI7Pao",
        "outputId": "677b4938-08a6-4971-9f3d-1699d343ab84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   transaction_id  user_id            date_time                city    amount  \\\n",
            "0               1     8936  2024-07-25 04:35:15               Delhi  14861.19   \n",
            "1               2     3612  2025-03-06 01:47:11              Jaipur  13172.75   \n",
            "2               3     5515  2024-11-12 03:03:06               Delhi   7134.89   \n",
            "3               4     2046  2025-04-10 10:58:50  Thiruvananthapuram   1347.28   \n",
            "4               5     1140  2025-05-01 15:41:25              Mumbai   7551.88   \n",
            "5               6     5978  2025-05-03 12:12:47           Bengaluru   2469.53   \n",
            "6               7     8752  2025-02-07 16:07:39              Bhopal   1892.35   \n",
            "7               8      917  2025-02-23 06:08:03             Chennai  11360.52   \n",
            "8               9     5156  2025-05-21 15:02:43             Chennai  18886.74   \n",
            "9              10     9578  2024-09-04 18:44:05           Ahmedabad  15027.29   \n",
            "\n",
            "  payment_method                  merchant  \\\n",
            "0  Mobile Wallet          Maharaj and Sons   \n",
            "1  Mobile Wallet  Chaudry, Chahal and Sami   \n",
            "2            UPI           Kamdar and Sons   \n",
            "3     Debit Card                   Sen PLC   \n",
            "4           Cash               Ahuja-Dutta   \n",
            "5    Credit Card                  Oak-Mall   \n",
            "6     NetBanking                  Kale LLC   \n",
            "7         Cheque                   Rai Inc   \n",
            "8           Cash                   Laborum   \n",
            "9  Mobile Wallet   Lalla, Bassi and Sharaf   \n",
            "\n",
            "                                 description  category  \n",
            "0    Amazon purchase of tempore items: culpa  Shopping  \n",
            "1    Amazon  purchasee of et items: deserunt  Shopping  \n",
            "2  Amazon purchase of aliquid items: aliquam  Shopping  \n",
            "3       Bala, Issac and Saini shopping spree  Shopping  \n",
            "4                            Monthly payment  Shopping  \n",
            "5                   Grocery haul: voluptatem  Shopping  \n",
            "6          Grocery shopping at {supermarket}  Shopping  \n",
            "7                       Purchase at corporis  Shopping  \n",
            "8                         Transfer to magnam  Shopping  \n",
            "9          Grocery shopping at {supermarket}  Shopping  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['category'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lC2Dk-L3tzm",
        "outputId": "bcad6ff1-2af5-4896-f506-75a28659d615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Shopping', 'Food & Dining', 'Transportation', 'Travel',\n",
              "       'Bills & Utilities', 'Entertainment', 'Healthcare', 'Housing',\n",
              "       'Personal Care', 'Insurance', 'Education', 'Financial Obligations',\n",
              "       'Miscellaneous', 'Taxes', 'Charity/Donations', 'Pets', 'Childcare'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts = df['category'].value_counts()\n",
        "print(\"Category counts:\\n\", category_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53T8vGRaSc2Z",
        "outputId": "54a27cfa-cc1f-42c6-a808-8216d98865e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category counts:\n",
            " category\n",
            "Shopping                 20000\n",
            "Food & Dining            18000\n",
            "Transportation           12000\n",
            "Bills & Utilities         9000\n",
            "Travel                    8000\n",
            "Entertainment             8000\n",
            "Healthcare                5000\n",
            "Housing                   5000\n",
            "Personal Care             4000\n",
            "Insurance                 3000\n",
            "Education                 3000\n",
            "Financial Obligations     3000\n",
            "Miscellaneous             2000\n",
            "Taxes                     2000\n",
            "Charity/Donations         1000\n",
            "Pets                      1000\n",
            "Childcare                 1000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)  # Ensure text is string\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "JrUUoiVZ8SX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_desc'] = df['description'].apply(clean_text)\n",
        "labels = df['category'].astype('category')\n",
        "df['label'] = labels.cat.codes\n",
        "label_mapping = dict(enumerate(labels.cat.categories))"
      ],
      "metadata": {
        "id": "3AbMonEn8eej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFCpJMzcU6K4",
        "outputId": "c6d6ea01-39c9-4119-ded4-9a41a8cf42f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Bills & Utilities', 1: 'Charity/Donations', 2: 'Childcare', 3: 'Education', 4: 'Entertainment', 5: 'Financial Obligations', 6: 'Food & Dining', 7: 'Healthcare', 8: 'Housing', 9: 'Insurance', 10: 'Miscellaneous', 11: 'Personal Care', 12: 'Pets', 13: 'Shopping', 14: 'Taxes', 15: 'Transportation', 16: 'Travel'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_desc'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
        ")"
      ],
      "metadata": {
        "id": "42HmR4BDEGPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=10000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss')\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "catboost = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "\n",
        "xgb.fit(X_train_tfidf, y_train)\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "catboost.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_xgb = xgb.predict(X_test_tfidf)\n",
        "y_pred_rf = rf.predict(X_test_tfidf)\n",
        "y_pred_catboost = catboost.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "CeH0nIECETc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "46d67eaf-bce7-4ab5-8acd-acf43474d326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3346102890.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcatboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1597\u001b[0m             )\n\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1600\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             _check_call(\n\u001b[0;32m-> 2101\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"=== XGBoost Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=labels.cat.categories))\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"=== Random Forest Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=labels.cat.categories))\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"=== CatBoost Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_catboost, target_names=labels.cat.categories))\n",
        "accuracy_catboost = accuracy_score(y_test, y_pred_catboost)"
      ],
      "metadata": {
        "id": "U6PDKoEqEczJ",
        "outputId": "ccd5f94a-13dc-47d0-b1d9-9a5dddc7a307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== XGBoost Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       0.97      0.79      0.87      1800\n",
            "    Charity/Donations       0.98      0.79      0.87       200\n",
            "            Childcare       0.97      0.71      0.82       200\n",
            "            Education       0.97      0.71      0.82       600\n",
            "        Entertainment       0.99      0.81      0.89      1600\n",
            "Financial Obligations       1.00      0.79      0.88       600\n",
            "        Food & Dining       0.63      0.89      0.74      3600\n",
            "           Healthcare       0.95      0.72      0.82      1000\n",
            "              Housing       0.98      0.81      0.89      1000\n",
            "            Insurance       0.99      0.77      0.87       600\n",
            "        Miscellaneous       0.99      0.70      0.82       400\n",
            "        Personal Care       0.96      0.74      0.84       800\n",
            "                 Pets       0.99      0.76      0.86       200\n",
            "             Shopping       0.70      0.89      0.78      4000\n",
            "                Taxes       0.99      0.82      0.90       400\n",
            "       Transportation       0.95      0.80      0.87      2400\n",
            "               Travel       0.98      0.79      0.87      1600\n",
            "\n",
            "             accuracy                           0.82     21000\n",
            "            macro avg       0.94      0.78      0.85     21000\n",
            "         weighted avg       0.86      0.82      0.83     21000\n",
            "\n",
            "=== Random Forest Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       0.91      0.80      0.85      1800\n",
            "    Charity/Donations       0.92      0.78      0.85       200\n",
            "            Childcare       0.94      0.72      0.82       200\n",
            "            Education       0.92      0.71      0.80       600\n",
            "        Entertainment       0.90      0.81      0.85      1600\n",
            "Financial Obligations       0.91      0.80      0.85       600\n",
            "        Food & Dining       0.74      0.84      0.79      3600\n",
            "           Healthcare       0.90      0.72      0.80      1000\n",
            "              Housing       0.90      0.81      0.85      1000\n",
            "            Insurance       0.94      0.78      0.85       600\n",
            "        Miscellaneous       0.83      0.74      0.79       400\n",
            "        Personal Care       0.90      0.75      0.81       800\n",
            "                 Pets       0.92      0.77      0.84       200\n",
            "             Shopping       0.70      0.89      0.78      4000\n",
            "                Taxes       0.94      0.83      0.88       400\n",
            "       Transportation       0.88      0.82      0.85      2400\n",
            "               Travel       0.89      0.80      0.84      1600\n",
            "\n",
            "             accuracy                           0.82     21000\n",
            "            macro avg       0.88      0.79      0.83     21000\n",
            "         weighted avg       0.83      0.82      0.82     21000\n",
            "\n",
            "=== CatBoost Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       0.99      0.78      0.87      1800\n",
            "    Charity/Donations       0.99      0.73      0.84       200\n",
            "            Childcare       1.00      0.67      0.80       200\n",
            "            Education       1.00      0.67      0.80       600\n",
            "        Entertainment       0.98      0.79      0.87      1600\n",
            "Financial Obligations       1.00      0.76      0.86       600\n",
            "        Food & Dining       0.61      0.88      0.72      3600\n",
            "           Healthcare       1.00      0.68      0.81      1000\n",
            "              Housing       0.99      0.79      0.88      1000\n",
            "            Insurance       0.97      0.75      0.85       600\n",
            "        Miscellaneous       1.00      0.67      0.80       400\n",
            "        Personal Care       0.71      0.72      0.71       800\n",
            "                 Pets       0.99      0.76      0.86       200\n",
            "             Shopping       0.68      0.90      0.78      4000\n",
            "                Taxes       0.97      0.79      0.87       400\n",
            "       Transportation       0.94      0.79      0.86      2400\n",
            "               Travel       0.99      0.76      0.86      1600\n",
            "\n",
            "             accuracy                           0.81     21000\n",
            "            macro avg       0.93      0.76      0.83     21000\n",
            "         weighted avg       0.85      0.81      0.81     21000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2]\n",
        "}\n",
        "grid_xgb = GridSearchCV(\n",
        "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
        "    param_grid_xgb,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=3\n",
        ")"
      ],
      "metadata": {
        "id": "cSkshFREUREW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_xgb.fit(X_train_tfidf, y_train)\n",
        "print(\"Best parameters for XGBoost:\", grid_xgb.best_params_)\n",
        "print(\"Best cross-validation accuracy for XGBoost:\", grid_xgb.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR8OKbMqEKxv",
        "outputId": "a6c73dca-1898-4c70-8b13-2bef23003685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.784 total time=  20.0s\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.783 total time=  21.4s\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.782 total time=  19.9s\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.791 total time=  41.7s\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.793 total time=  42.3s\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.786 total time=  41.8s\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.804 total time= 1.5min\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.802 total time= 1.4min\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.796 total time= 1.4min\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.808 total time=  34.7s\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.808 total time=  36.5s\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.799 total time=  34.8s\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.811 total time= 1.3min\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.810 total time= 1.2min\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.803 total time= 1.3min\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.812 total time= 2.6min\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.813 total time= 2.6min\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.805 total time= 2.7min\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=7, n_estimators=50;, score=0.805 total time=  48.7s\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=7, n_estimators=50;, score=0.807 total time=  49.7s\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=7, n_estimators=50;, score=0.799 total time=  48.2s\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.808 total time= 1.8min\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.812 total time= 1.7min\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.803 total time= 1.7min\n",
            "[CV 1/3] END learning_rate=0.01, max_depth=7, n_estimators=200;, score=0.814 total time= 3.7min\n",
            "[CV 2/3] END learning_rate=0.01, max_depth=7, n_estimators=200;, score=0.814 total time= 3.7min\n",
            "[CV 3/3] END learning_rate=0.01, max_depth=7, n_estimators=200;, score=0.807 total time= 3.7min\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.812 total time=  21.0s\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.812 total time=  19.4s\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.805 total time=  21.0s\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.816 total time=  37.2s\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.817 total time=  39.1s\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.810 total time=  38.9s\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.819 total time= 1.2min\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.821 total time= 1.2min\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.814 total time= 1.2min\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.816 total time=  36.8s\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.817 total time=  37.4s\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.810 total time=  36.3s\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.819 total time= 1.1min\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.822 total time= 1.1min\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.813 total time= 1.1min\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.821 total time= 2.0min\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.821 total time= 2.0min\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.815 total time= 2.0min\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=7, n_estimators=50;, score=0.818 total time=  52.4s\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=7, n_estimators=50;, score=0.820 total time=  53.2s\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=7, n_estimators=50;, score=0.813 total time=  52.7s\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.820 total time= 1.7min\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.821 total time= 1.7min\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.815 total time= 1.6min\n",
            "[CV 1/3] END learning_rate=0.1, max_depth=7, n_estimators=200;, score=0.820 total time= 3.0min\n",
            "[CV 2/3] END learning_rate=0.1, max_depth=7, n_estimators=200;, score=0.821 total time= 3.0min\n",
            "[CV 3/3] END learning_rate=0.1, max_depth=7, n_estimators=200;, score=0.814 total time= 2.9min\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.816 total time=  19.5s\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.817 total time=  18.4s\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.810 total time=  19.8s\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.819 total time=  34.8s\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.820 total time=  36.2s\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.814 total time=  34.3s\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.820 total time= 1.1min\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.821 total time= 1.1min\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.814 total time= 1.1min\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.820 total time=  33.9s\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.821 total time=  32.7s\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.813 total time=  33.3s\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.821 total time= 1.0min\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.821 total time=  59.8s\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.815 total time=  59.8s\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.819 total time= 1.8min\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.820 total time= 1.9min\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.813 total time= 1.8min\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=7, n_estimators=50;, score=0.820 total time=  48.7s\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=7, n_estimators=50;, score=0.821 total time=  48.5s\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=7, n_estimators=50;, score=0.814 total time=  48.5s\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=7, n_estimators=100;, score=0.820 total time= 1.5min\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=7, n_estimators=100;, score=0.820 total time= 1.5min\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=7, n_estimators=100;, score=0.814 total time= 1.4min\n",
            "[CV 1/3] END learning_rate=0.2, max_depth=7, n_estimators=200;, score=0.818 total time= 2.6min\n",
            "[CV 2/3] END learning_rate=0.2, max_depth=7, n_estimators=200;, score=0.819 total time= 2.6min\n",
            "[CV 3/3] END learning_rate=0.2, max_depth=7, n_estimators=200;, score=0.812 total time= 2.6min\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
            "Best cross-validation accuracy for XGBoost: 0.8190238095238095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(grid_xgb.best_estimator_, os.path.join(save_dir, 'xgb_best_model.pkl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWbGCGEWhM7t",
        "outputId": "3e35c40f-6f7f-4d36-a5c3-71df33aa9f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/expense_categorization/xgb_best_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(os.path.join(save_dir, 'label_mapping.pkl'), 'wb') as f:\n",
        "    pickle.dump(label_mapping, f)"
      ],
      "metadata": {
        "id": "syi3MB47hwW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(description, model_name, model_path, vectorizer_path, label_mapping_path):\n",
        "    # Load the model, vectorizer, and label mapping\n",
        "    model = joblib.load(model_path)\n",
        "    tfidf = joblib.load(vectorizer_path)\n",
        "    with open(label_mapping_path, 'rb') as f:\n",
        "        label_mapping = pickle.load(f)\n",
        "\n",
        "    # Clean and transform the description\n",
        "    clean_desc = clean_text(description)\n",
        "    desc_tfidf = tfidf.transform([clean_desc])\n",
        "\n",
        "    # Predict\n",
        "    predicted_label = model.predict(desc_tfidf)[0]\n",
        "    category = label_mapping[predicted_label]\n",
        "    return category\n"
      ],
      "metadata": {
        "id": "AS0JbNSF8jhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ],
      "metadata": {
        "id": "wYWKCrpgz364"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_transformer(model_name, train_texts, train_labels, test_texts, test_labels):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=len(label_mapping)\n",
        "    )\n",
        "\n",
        "    # Tokenize\n",
        "    train_encodings = tokenizer(\n",
        "        train_texts.tolist(), truncation=True, padding=True, max_length=128\n",
        "    )\n",
        "    test_encodings = tokenizer(\n",
        "        test_texts.tolist(), truncation=True, padding=True, max_length=128\n",
        "    )\n",
        "\n",
        "    class ExpenseDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "        def __getitem__(self, idx):\n",
        "            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
        "            return item\n",
        "\n",
        "    train_dataset = ExpenseDataset(train_encodings, train_labels.reset_index(drop=True))\n",
        "    eval_dataset = ExpenseDataset(test_encodings, test_labels.reset_index(drop=True))\n",
        "\n",
        "    # TrainingArguments\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'./results-{model_name.split(\"/\")[-1]}',\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        eval_strategy='epoch',  # Updated from evaluation_strategy\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=2e-5,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"=== {model_name} Fine-Tuning Results ===\")\n",
        "    print(metrics)\n",
        "\n",
        "    # Predictions\n",
        "    preds = trainer.predict(eval_dataset)\n",
        "    y_pred = np.argmax(preds.predictions, axis=1)\n",
        "    print(f\"=== {model_name} Classification Report ===\")\n",
        "    print(classification_report(test_labels, y_pred, target_names=labels.cat.categories))\n",
        "    accuracy = accuracy_score(test_labels, y_pred)\n",
        "    print(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    model.save_pretrained(os.path.join(save_dir, f'{model_name.split(\"/\")[-1]}_finetuned'))\n",
        "    tokenizer.save_pretrained(os.path.join(save_dir, f'{model_name.split(\"/\")[-1]}_finetuned'))\n",
        "    return model, tokenizer, y_pred, accuracy\n"
      ],
      "metadata": {
        "id": "vCsbeabmZNiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model, roberta_tok, y_pred_roberta, accuracy_roberta = fine_tune_transformer(\n",
        "    'roberta-base', X_train, y_train, X_test, y_test\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898,
          "referenced_widgets": [
            "2571dc018aba4ab3a5a0a943162fb5eb",
            "e5a91c0e858f427d9b95bf9e9b792d87",
            "5f6701b9676e44a0b16e4972d22735ef",
            "9f402232adb147ef9c860f1787de10c6",
            "65bde4afcf88496092cef8ac27f964d3",
            "1a6f5482fdf44f0e93967df11e8cc18b",
            "df6287ee993d4ce3b7d94e2d6955828f",
            "a56e72510242453680ddc494866196a2",
            "71a84b6aed224da0a00a2b0b784a804b",
            "bd3c8f4a90444c9ab777d51282bba960",
            "66f8784194ba46e4a6b25a9383ea8f3b",
            "6d9ef20235ed4653b759f08bf7261ca9",
            "f0a7710b2767417f9ed3243902288570",
            "d27394fdd4c5485585f94c8b388b5f9e",
            "0dbf8588106a432fb109fe1ef1e18990",
            "d802ad33db7c4111873ec8cf7ea2d7f5",
            "4a0d1f15fb994a4aa2289fa8d24f4bd8",
            "3dae14ecc47d477192793a4c933d5a46",
            "49de139ae49f45a58c581678ede9bc15",
            "0e2f83ecd5b340ef80f87366a3f52d30",
            "76590c74434c4513bfa6a5ba61a5a083",
            "1bbf868e95bb4b04a894b31f8b5449cd",
            "eb81b4fd58db4136a1531e6a46a54399",
            "4211318a92bc454182ee355e51c43ced",
            "ea44ffb0fd674ba7b93e71179c33458f",
            "b7f27d0c7ff44a58a448468eb6d1f8b5",
            "1b009d8699af45f690a4556a5f4f523d",
            "f80608c8b3ca461da8de47b8620d693a",
            "e2092f090475450e96b2d2d98b5a7059",
            "be1a785fd76c4da18db2722ae65dd896",
            "0adb70d345d742f29a86f6c18e864c3d",
            "a44a7e92765043f095ab11ae839d8680",
            "72df6b17d5684cc1bd2f26680dd1e308",
            "e1f42206dd5e47828f3e1cf87b57c3e8",
            "3c2de7cd622e4a28bb22bd1fabc366df",
            "ce7aadff38fc4fdfacd132d968e16cb8",
            "408303b1b0c84ce9a37ebd8f72db7af4",
            "1de95bb52b6b4966a811d8a211016d21",
            "e5ca4e8e70de4ed5b511a85d162c1497",
            "3e79b8c2647f4068b8d7f96a20948dd5",
            "be1456c76b444778aa44569c470dadfb",
            "0fb1217163da4035a95419a65b41798c",
            "e141e5fdad9c4f84b7837d1da6274c6d",
            "61f2b646bd2a432a90e1812e3033c86b",
            "0af002033df249d78478726aca83db4b",
            "8eb3466546c24159aefe6714055193c4",
            "18372017a7d043ce987363b042d02c11",
            "56f49898921d4a9db9a138f6da650123",
            "fa93823aacb74f44ade7ca56f09068f2",
            "fb8bf0d2defa4f4d99bd891849e3e8a7",
            "9de31e89b7d74e01af4dc51074c586c3",
            "579c32af79f54c98b21213dd5f316704",
            "a9bac8d5f1cc4fc6b9de6dc6c2f1a23c",
            "cb2fa477f4fb4044b6010bf6b43d1e99",
            "151f734121ba4e9da5e7b822a8b62dff",
            "f7778f33dbf04c199749f57e8b14d53c",
            "32c2b6925aa34d1f87ec7078504e0a91",
            "3562a20d5e3244d69f7c81cc10ad032f",
            "e48018ce9036407a9c172f8e365476cb",
            "2eecc8c929c6436688b228edaf35e44c",
            "333fb80687344632b40da6cd87ff03a2",
            "724cdf29efa340a792a39ea90bfe7151",
            "5b00163e62a64bcbadb92dfa34647129",
            "302db279107e49a29234571bb8a4b482",
            "a09d9ff765c248f1953d3ebe84e62e55",
            "438bcfa17a864faa919abc7dfcac9853"
          ]
        },
        "id": "M7Dq7g6mwnLp",
        "outputId": "a81b4a5a-2a06-4f60-8ea5-454f2caae748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2571dc018aba4ab3a5a0a943162fb5eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d9ef20235ed4653b759f08bf7261ca9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb81b4fd58db4136a1531e6a46a54399"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1f42206dd5e47828f3e1cf87b57c3e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0af002033df249d78478726aca83db4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7778f33dbf04c199749f57e8b14d53c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10500' max='10500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10500/10500 21:48, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.579100</td>\n",
              "      <td>0.539313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.555700</td>\n",
              "      <td>0.533138</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== roberta-base Fine-Tuning Results ===\n",
            "{'eval_loss': 0.5331382751464844, 'eval_runtime': 32.5395, 'eval_samples_per_second': 645.369, 'eval_steps_per_second': 20.191, 'epoch': 2.0}\n",
            "=== roberta-base Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       1.00      0.79      0.88      1800\n",
            "    Charity/Donations       0.99      0.80      0.89       200\n",
            "            Childcare       1.00      0.73      0.85       200\n",
            "            Education       1.00      0.71      0.83       600\n",
            "        Entertainment       1.00      0.81      0.89      1600\n",
            "Financial Obligations       1.00      0.80      0.88       600\n",
            "        Food & Dining       0.87      0.81      0.84      3600\n",
            "           Healthcare       0.99      0.72      0.83      1000\n",
            "              Housing       1.00      0.81      0.90      1000\n",
            "            Insurance       1.00      0.78      0.88       600\n",
            "        Miscellaneous       1.00      0.79      0.88       400\n",
            "        Personal Care       0.98      0.75      0.85       800\n",
            "                 Pets       1.00      0.78      0.88       200\n",
            "             Shopping       0.56      0.99      0.71      4000\n",
            "                Taxes       1.00      0.83      0.91       400\n",
            "       Transportation       1.00      0.80      0.89      2400\n",
            "               Travel       1.00      0.79      0.88      1600\n",
            "\n",
            "             accuracy                           0.83     21000\n",
            "            macro avg       0.96      0.79      0.86     21000\n",
            "         weighted avg       0.89      0.83      0.84     21000\n",
            "\n",
            "roberta-base Test Accuracy: 0.8279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_finbert(model_name, train_texts, train_labels, test_texts, test_labels):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(label_mapping),\n",
        "        ignore_mismatched_sizes=True  # Fix for FinBERT size mismatch\n",
        "    )\n",
        "\n",
        "    # Tokenize\n",
        "    train_encodings = tokenizer(\n",
        "        train_texts.tolist(), truncation=True, padding=True, max_length=128\n",
        "    )\n",
        "    test_encodings = tokenizer(\n",
        "        test_texts.tolist(), truncation=True, padding=True, max_length=128\n",
        "    )\n",
        "\n",
        "    # Dataset class\n",
        "    class ExpenseDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "        def __getitem__(self, idx):\n",
        "            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
        "            return item\n",
        "\n",
        "    train_dataset = ExpenseDataset(train_encodings, train_labels.reset_index(drop=True))\n",
        "    eval_dataset = ExpenseDataset(test_encodings, test_labels.reset_index(drop=True))\n",
        "\n",
        "    # TrainingArguments\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'./results-{model_name.split(\"/\")[-1]}',\n",
        "        num_train_epochs=4,  # Set to 4 epochs as per previous request\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=2e-5,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='eval_loss',\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"=== {model_name} Fine-Tuning Results ===\")\n",
        "    print(metrics)\n",
        "\n",
        "    # Predictions\n",
        "    preds = trainer.predict(eval_dataset)\n",
        "    y_pred = np.argmax(preds.predictions, axis=1)\n",
        "    print(f\"=== {model_name} Classification Report ===\")\n",
        "    print(classification_report(test_labels, y_pred, target_names=label_mapping.values()))\n",
        "    accuracy = accuracy_score(test_labels, y_pred)\n",
        "    print(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    model.save_pretrained(os.path.join(save_dir, f'{model_name.split(\"/\")[-1]}_finetuned'))\n",
        "    tokenizer.save_pretrained(os.path.join(save_dir, f'{model_name.split(\"/\")[-1]}_finetuned'))\n",
        "    return model, tokenizer, y_pred, accuracy"
      ],
      "metadata": {
        "id": "BZkcmjxoGU4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finbert_model, finbert_tok, y_pred_finbert, accuracy_finbert = fine_tune_finbert(\n",
        "    'yiyanghkust/finbert-tone', X_train, y_train, X_test, y_test\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "dvflgoaWwuXL",
        "outputId": "971aa4fb-6edc-4d8a-aaff-b88ab207870d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-tone and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([17, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([17]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17470' max='21000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [17470/21000 34:34 < 06:59, 8.42 it/s, Epoch 3.33/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.572800</td>\n",
              "      <td>0.542568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.545900</td>\n",
              "      <td>0.538309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.518100</td>\n",
              "      <td>0.536951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21000' max='21000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21000/21000 41:47, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.572800</td>\n",
              "      <td>0.542568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.545900</td>\n",
              "      <td>0.538309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.518100</td>\n",
              "      <td>0.536951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.524400</td>\n",
              "      <td>0.546834</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== yiyanghkust/finbert-tone Fine-Tuning Results ===\n",
            "{'eval_loss': 0.5369514226913452, 'eval_runtime': 29.0203, 'eval_samples_per_second': 723.631, 'eval_steps_per_second': 22.639, 'epoch': 4.0}\n",
            "=== yiyanghkust/finbert-tone Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       1.00      0.79      0.88      1800\n",
            "    Charity/Donations       0.99      0.80      0.88       200\n",
            "            Childcare       0.99      0.72      0.84       200\n",
            "            Education       1.00      0.71      0.83       600\n",
            "        Entertainment       1.00      0.81      0.89      1600\n",
            "Financial Obligations       1.00      0.80      0.89       600\n",
            "        Food & Dining       0.86      0.81      0.83      3600\n",
            "           Healthcare       0.99      0.72      0.84      1000\n",
            "              Housing       1.00      0.81      0.90      1000\n",
            "            Insurance       1.00      0.78      0.88       600\n",
            "        Miscellaneous       1.00      0.78      0.88       400\n",
            "        Personal Care       0.99      0.74      0.85       800\n",
            "                 Pets       0.99      0.78      0.87       200\n",
            "             Shopping       0.56      0.99      0.71      4000\n",
            "                Taxes       1.00      0.83      0.90       400\n",
            "       Transportation       1.00      0.80      0.89      2400\n",
            "               Travel       1.00      0.79      0.88      1600\n",
            "\n",
            "             accuracy                           0.83     21000\n",
            "            macro avg       0.96      0.79      0.86     21000\n",
            "         weighted avg       0.89      0.83      0.84     21000\n",
            "\n",
            "yiyanghkust/finbert-tone Test Accuracy: 0.8275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_transformer(model_name, train_texts, train_labels, test_texts, test_labels, label_mapping):\n",
        "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "   model = AutoModelForSequenceClassification.from_pretrained(\n",
        "       model_name,\n",
        "       num_labels=len(label_mapping),\n",
        "       ignore_mismatched_sizes=True  # Handle size mismatches for classification head\n",
        "   )\n",
        "   # Tokenize\n",
        "   train_encodings = tokenizer(\n",
        "       train_texts.tolist(), truncation=True, padding=True, max_length=128\n",
        "   )\n",
        "   test_encodings = tokenizer(\n",
        "       test_texts.tolist(), truncation=True, padding=True, max_length=128\n",
        "   )\n",
        "   # Dataset class\n",
        "   class ExpenseDataset(torch.utils.data.Dataset):\n",
        "       def __init__(self, encodings, labels):\n",
        "           self.encodings = encodings\n",
        "           self.labels = labels\n",
        "       def __len__(self):\n",
        "           return len(self.labels)\n",
        "       def __getitem__(self, idx):\n",
        "           item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "           item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
        "           return item\n",
        "   train_dataset = ExpenseDataset(train_encodings, train_labels.reset_index(drop=True))\n",
        "   eval_dataset = ExpenseDataset(test_encodings, test_labels.reset_index(drop=True))\n",
        "   # TrainingArguments\n",
        "   args = TrainingArguments(\n",
        "       output_dir=f'./results-{model_name.split(\"/\")[-1]}',\n",
        "       num_train_epochs=3,\n",
        "       per_device_train_batch_size=16,\n",
        "       per_device_eval_batch_size=32,\n",
        "       eval_strategy='epoch',\n",
        "       save_strategy='epoch',\n",
        "       learning_rate=2e-5,\n",
        "       logging_dir='./logs',\n",
        "       logging_steps=50,\n",
        "       load_best_model_at_end=True,\n",
        "       metric_for_best_model='eval_loss',\n",
        "       report_to=\"none\"\n",
        "   )\n",
        "   trainer = Trainer(\n",
        "       model=model,\n",
        "       args=args,\n",
        "       train_dataset=train_dataset,\n",
        "       eval_dataset=eval_dataset,\n",
        "       tokenizer=tokenizer,\n",
        "   )\n",
        "   trainer.train()\n",
        "   metrics = trainer.evaluate()\n",
        "   print(f\"=== {model_name} Fine-Tuning Results ===\")\n",
        "   print(metrics)\n",
        "   # Predictions\n",
        "   preds = trainer.predict(eval_dataset)\n",
        "   y_pred = np.argmax(preds.predictions, axis=1)\n",
        "   print(f\"=== {model_name} Classification Report ===\")\n",
        "   print(classification_report(test_labels, y_pred, target_names=label_mapping.values()))\n",
        "   accuracy = accuracy_score(test_labels, y_pred)\n",
        "   print(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
        "   # Save model and tokenizer\n",
        "   model.save_pretrained(os.path.join(save_dir, f'{model_name.split(\"/\")[-1]}_finetuned'))\n",
        "   tokenizer.save_pretrained(os.path.join(save_dir, f'{model_name.split(\"/\")[-1]}_finetuned'))\n",
        "   return model, tokenizer, y_pred, accuracy\n",
        "\n",
        "models_to_test = [\n",
        "    'microsoft/deberta-base',\n",
        "     'google/electra-base-discriminator',\n",
        "    'albert-base-v2',\n",
        "    'bert-base-uncased'\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Fine-tune all models\n",
        "results = {}\n",
        "for model_name in models_to_test:\n",
        "    print(f\"\\nFine-tuning {model_name}...\")\n",
        "    model, tokenizer, y_pred, accuracy = fine_tune_transformer(\n",
        "        model_name, X_train, y_train, X_test, y_test, label_mapping\n",
        "    )\n",
        "    results[model_name] = {\n",
        "        'model': model,\n",
        "        'tokenizer': tokenizer,\n",
        "        'y_pred': y_pred,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "\n",
        "# Print summary of accuracies\n",
        "print(\"\\n=== Model Accuracy Comparison ===\")\n",
        "for model_name, result in results.items():\n",
        "    print(f\"{model_name}: Test Accuracy = {result['accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "76eb98a2c3e24e4394d7bc616b8e61d9",
            "5629bba1b4194503896458e535b0b1b1",
            "6993a68081614f3fbdc946bbed3be2d2",
            "7ce09c8572d04fae8c08ed52926ac46e",
            "b73735e9a299434b927b86978c7197ee",
            "9238b75edecf44728389f68e2e68299e",
            "d7b3af02eed6480eaeb7e268f0082627",
            "2531310c2f2040468159eb4eeb5ca6fc",
            "a6d9455396394d00a8278f165e37ea8b",
            "695ae9e2f35f4f399cd92d2dc34374a7",
            "15e063b396bb4bf3866e6f6debd7767f",
            "bb2e0bdb76924049b72baa7d284612cb",
            "036a580fd5944b4da3a6e00bda67e0f3",
            "d6b0166403264b3da5aa3fdeaa9c6b14",
            "7d858f6459e64180820d0eafe734829c",
            "6cb62cebb79f4fd784cdd0e2d98e901c",
            "bebbb98a0c2e4c7696d0e889801bfdbe",
            "d6f3d95f6a1c43abbbf5c4aa429156c4",
            "6e26e10c02ca4c679e569d59b551682d",
            "ba8a13e299704c998e3150b3c345e40e",
            "0487484a6c0f4274bcafb01ceea1a45c",
            "7329b8edc01645119d5ff211730f2be9",
            "9a56b3b715d646d6adf19140fcbc6c28",
            "3e802d7db20245ba8e3d6fc467c576b9",
            "9f8b14b3b4584fd5929cf5e729e5445e",
            "3ede680141f340f2a1689985533500c3",
            "a4a78255c98b4ac084b721432ef2b9e6",
            "3c367f614fe14ff8bd45bbd38a9fc4db",
            "506e42f6a6c9481e9c4d1aaad8829041",
            "406eff8b7cba4b328865c9ae652091df",
            "37797f06aa3b4ae69e7b83fc28e3bb7c",
            "8d65a222191e4b06b707cd18f24c50b5",
            "92fe52f09d9f4bfe9301b7086304b6dd",
            "adcd93f648f94bfca981eadc60eea374",
            "ee06da6d47234989b26b664289bf609e",
            "b99ea84be6b6423082580e82c06dd9fa",
            "eeaa94d96dd246cbb6df55d6726cd974",
            "dc631a2a3269426aaa814879445a0ba4",
            "ff4effce9296471098fc52cae2e34911",
            "77030612966c442babdd2bcd6a6c41c5",
            "5fdff608cdb44f35b7799723c691b6e1",
            "6e1ce7582aff483e90b1ef0fe2474669",
            "3802a0dba7a04ae788a11c300199069d",
            "6cfd4cd5f66e40b188bd52fb5552cab8",
            "4263fb43d9f240a7bbd6da2a52c8da9e",
            "9daf68bef5bd4f7398216264c3b2ab17",
            "d8ca8b2276174801a4071b535dd5fcf4",
            "d07c96b3e02e4262ac26dd4cf6490448",
            "74c93597d2ea4df9886bd17bfbe0deaa",
            "505f58483d8f4e68966aa030e6282e03",
            "6718240eeb81489f8109d5d58ae97195",
            "94d9ce01abe44aa69746c351041dc26b",
            "9dd6f32c12904745951d6520b31d095e",
            "d96cb80429ba4a5187cb36e265acdf4d",
            "66658cec71bf4b7e82894b52a4bc7b09",
            "81280c952e914fd3805d0a1371fe43e8",
            "97e28f3280cd4d02b4ae2b5fd458e582",
            "b86815e0a7794668a61588e6d7e03266",
            "6969dca98ee84cd3ba3e95f4a3841425",
            "dbcd75d1dfca46528a50b2a5b249aa13",
            "df4d4ade10be4deaa030eba5e92a5700",
            "5d607e18557041938d8f0943d8531957",
            "910de5b87fa840919948fe8ecea5e4d7",
            "e66a7f67b95649a5941131fb3291e144",
            "082d965c19194fedba0924d4fda30261",
            "2f3fe3154afb4bbcb3c33cb43c4f9593"
          ]
        },
        "id": "BS_YVF9r89d1",
        "outputId": "d57c6b4e-753e-4071-9dfe-1d49f056fadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fine-tuning microsoft/deberta-base...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76eb98a2c3e24e4394d7bc616b8e61d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb2e0bdb76924049b72baa7d284612cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a56b3b715d646d6adf19140fcbc6c28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adcd93f648f94bfca981eadc60eea374",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4263fb43d9f240a7bbd6da2a52c8da9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81280c952e914fd3805d0a1371fe43e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4170' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4170/15750 10:09 < 28:13, 6.84 it/s, Epoch 0.79/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15750/15750 57:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.566900</td>\n",
              "      <td>0.541965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.547000</td>\n",
              "      <td>0.534086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.515700</td>\n",
              "      <td>0.532876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== microsoft/deberta-base Fine-Tuning Results ===\n",
            "{'eval_loss': 0.5328761339187622, 'eval_runtime': 40.3883, 'eval_samples_per_second': 519.952, 'eval_steps_per_second': 16.267, 'epoch': 3.0}\n",
            "=== microsoft/deberta-base Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       1.00      0.79      0.88      1800\n",
            "    Charity/Donations       0.99      0.80      0.89       200\n",
            "            Childcare       1.00      0.73      0.84       200\n",
            "            Education       1.00      0.71      0.83       600\n",
            "        Entertainment       1.00      0.81      0.89      1600\n",
            "Financial Obligations       1.00      0.80      0.89       600\n",
            "        Food & Dining       0.88      0.81      0.84      3600\n",
            "           Healthcare       0.94      0.73      0.82      1000\n",
            "              Housing       1.00      0.81      0.90      1000\n",
            "            Insurance       1.00      0.78      0.88       600\n",
            "        Miscellaneous       1.00      0.79      0.88       400\n",
            "        Personal Care       0.99      0.75      0.85       800\n",
            "                 Pets       1.00      0.78      0.88       200\n",
            "             Shopping       0.56      0.99      0.71      4000\n",
            "                Taxes       1.00      0.83      0.91       400\n",
            "       Transportation       1.00      0.80      0.89      2400\n",
            "               Travel       1.00      0.79      0.88      1600\n",
            "\n",
            "             accuracy                           0.83     21000\n",
            "            macro avg       0.96      0.79      0.86     21000\n",
            "         weighted avg       0.89      0.83      0.84     21000\n",
            "\n",
            "microsoft/deberta-base Test Accuracy: 0.8279\n",
            "\n",
            "Fine-tuning google/electra-base-discriminator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15750/15750 40:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.566800</td>\n",
              "      <td>0.544279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.544100</td>\n",
              "      <td>0.536840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.517400</td>\n",
              "      <td>0.536025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== google/electra-base-discriminator Fine-Tuning Results ===\n",
            "{'eval_loss': 0.5360249876976013, 'eval_runtime': 35.3972, 'eval_samples_per_second': 593.267, 'eval_steps_per_second': 18.561, 'epoch': 3.0}\n",
            "=== google/electra-base-discriminator Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       1.00      0.79      0.88      1800\n",
            "    Charity/Donations       0.99      0.79      0.88       200\n",
            "            Childcare       1.00      0.72      0.84       200\n",
            "            Education       1.00      0.70      0.83       600\n",
            "        Entertainment       1.00      0.81      0.89      1600\n",
            "Financial Obligations       1.00      0.80      0.89       600\n",
            "        Food & Dining       0.88      0.81      0.84      3600\n",
            "           Healthcare       0.94      0.74      0.82      1000\n",
            "              Housing       1.00      0.81      0.90      1000\n",
            "            Insurance       1.00      0.78      0.88       600\n",
            "        Miscellaneous       1.00      0.78      0.87       400\n",
            "        Personal Care       0.99      0.74      0.85       800\n",
            "                 Pets       1.00      0.78      0.87       200\n",
            "             Shopping       0.56      0.99      0.71      4000\n",
            "                Taxes       1.00      0.83      0.91       400\n",
            "       Transportation       1.00      0.80      0.89      2400\n",
            "               Travel       1.00      0.79      0.88      1600\n",
            "\n",
            "             accuracy                           0.83     21000\n",
            "            macro avg       0.96      0.79      0.86     21000\n",
            "         weighted avg       0.89      0.83      0.84     21000\n",
            "\n",
            "google/electra-base-discriminator Test Accuracy: 0.8275\n",
            "\n",
            "Fine-tuning albert-base-v2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15750/15750 27:05, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.597400</td>\n",
              "      <td>0.547637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.559800</td>\n",
              "      <td>0.538572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.522700</td>\n",
              "      <td>0.536255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== albert-base-v2 Fine-Tuning Results ===\n",
            "{'eval_loss': 0.5362552404403687, 'eval_runtime': 37.569, 'eval_samples_per_second': 558.971, 'eval_steps_per_second': 17.488, 'epoch': 3.0}\n",
            "=== albert-base-v2 Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       1.00      0.79      0.88      1800\n",
            "    Charity/Donations       0.99      0.80      0.89       200\n",
            "            Childcare       1.00      0.72      0.84       200\n",
            "            Education       0.99      0.70      0.82       600\n",
            "        Entertainment       1.00      0.81      0.89      1600\n",
            "Financial Obligations       1.00      0.80      0.89       600\n",
            "        Food & Dining       0.88      0.81      0.84      3600\n",
            "           Healthcare       0.95      0.73      0.83      1000\n",
            "              Housing       1.00      0.81      0.90      1000\n",
            "            Insurance       1.00      0.78      0.88       600\n",
            "        Miscellaneous       1.00      0.78      0.88       400\n",
            "        Personal Care       0.98      0.74      0.85       800\n",
            "                 Pets       0.99      0.77      0.87       200\n",
            "             Shopping       0.56      0.99      0.71      4000\n",
            "                Taxes       0.99      0.83      0.90       400\n",
            "       Transportation       1.00      0.80      0.89      2400\n",
            "               Travel       1.00      0.79      0.88      1600\n",
            "\n",
            "             accuracy                           0.83     21000\n",
            "            macro avg       0.96      0.79      0.86     21000\n",
            "         weighted avg       0.89      0.83      0.84     21000\n",
            "\n",
            "albert-base-v2 Test Accuracy: 0.8273\n",
            "\n",
            "Fine-tuning bert-base-uncased...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15750/15750 34:16, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.581300</td>\n",
              "      <td>0.539919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.554100</td>\n",
              "      <td>0.536660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.510900</td>\n",
              "      <td>0.535694</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== bert-base-uncased Fine-Tuning Results ===\n",
            "{'eval_loss': 0.5356939435005188, 'eval_runtime': 34.2178, 'eval_samples_per_second': 613.716, 'eval_steps_per_second': 19.201, 'epoch': 3.0}\n",
            "=== bert-base-uncased Classification Report ===\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "    Bills & Utilities       1.00      0.79      0.88      1800\n",
            "    Charity/Donations       0.99      0.80      0.88       200\n",
            "            Childcare       1.00      0.72      0.84       200\n",
            "            Education       1.00      0.71      0.83       600\n",
            "        Entertainment       1.00      0.81      0.89      1600\n",
            "Financial Obligations       1.00      0.80      0.88       600\n",
            "        Food & Dining       0.88      0.80      0.84      3600\n",
            "           Healthcare       0.93      0.74      0.83      1000\n",
            "              Housing       1.00      0.81      0.90      1000\n",
            "            Insurance       1.00      0.78      0.88       600\n",
            "        Miscellaneous       1.00      0.78      0.87       400\n",
            "        Personal Care       0.96      0.75      0.84       800\n",
            "                 Pets       1.00      0.78      0.87       200\n",
            "             Shopping       0.56      0.99      0.71      4000\n",
            "                Taxes       1.00      0.83      0.91       400\n",
            "       Transportation       1.00      0.80      0.89      2400\n",
            "               Travel       1.00      0.79      0.88      1600\n",
            "\n",
            "             accuracy                           0.83     21000\n",
            "            macro avg       0.96      0.79      0.86     21000\n",
            "         weighted avg       0.89      0.83      0.84     21000\n",
            "\n",
            "bert-base-uncased Test Accuracy: 0.8278\n",
            "\n",
            "=== Model Accuracy Comparison ===\n",
            "microsoft/deberta-base: Test Accuracy = 0.8279\n",
            "google/electra-base-discriminator: Test Accuracy = 0.8275\n",
            "albert-base-v2: Test Accuracy = 0.8273\n",
            "bert-base-uncased: Test Accuracy = 0.8278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qqDt_0SlW5Q2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}